{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semi-supervised Learning Tutorial\n",
    "\n",
    "## The tutorial will use the RSSL package (https://cran.r-project.org/web/packages/RSSL/RSSL.pdf), and illustrate the algorithm of self-training and semi-supervised SVM. More details about the package can be found at: https://arxiv.org/pdf/1612.07993.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install.packages(\"RSSL\")\n",
    "#install.packages(\"dplyr\")\n",
    "#install.packages(\"tidyr\")\n",
    "#install.packages(\"ggplot2\")\n",
    "# he package \"RSSL\" can also be installed with the following command:\n",
    "#library(devtools)\n",
    "#install_github(\"jkrijthe/RSSL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Self-training with Nearest Mean Classifier\n",
    "In this task, the data comes from 2 class Gaussian distribution, we will show how to use self training to boost the performance of a Nearest Neighbour classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(RSSL)\n",
    "library(dplyr,warn.conflicts = FALSE)\n",
    "library(ggplot2,warn.conflicts = FALSE)\n",
    "library(tidyr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we set a random seed and generate 200 data points from two classes, with the variance as 0.2. Second, we randomly remove some labels to get the simulated unlabeled set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(2)\n",
    "df <- generate2ClassGaussian(200, d=2, var = 0.2, expected=TRUE)\n",
    "\n",
    "# Randomly remove labels\n",
    "df <- df %>% add_missinglabels_mar(Class~.,prob=0.98) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, can you initilize a Nearest Mean Classifier and do self training for it ?  We will see the difference of the decision boundary after incorperating the unlabeled set. The detailed code of the module SelfLearning can be found at: https://rdrr.io/cran/RSSL/src/R/SelfLearning.R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train classifier: enter your code here\n",
    "g_nm <- \n",
    "g_self <-\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the supervised v.s. semi-supervised model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate performance: Squared Loss & Error Rate\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The squared loss of the self trained classifier is smaller than that of supervised model. In other words, unlabeleld set helps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Semi-supervised learning with SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method (S4VM) randomly generates multiple low-density separators (controlled by the sample_time parameter) and merges their predictions by solving a linear programming problem meant to penalize the cost of decreasing the performance of the classifier, compared to the supervised SVM. S4VM is a bit of a misnomer, since it is a transductive method that only returns predicted labels for the unlabeled objects. The main difference in this implementation compared to the original implementation is the clustering of the low-density separators: in our implementation empty clusters are not dropped during the k-means procedure. In the paper by Li (2011) the features are first normalized to [0,1], which is not automatically done by this function. Note that the solution may not correspond to a linear classifier even if the linear kernel is used. More details can be found at: https://rdrr.io/cran/RSSL/man/S4VM.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(1)\n",
    "df_orig <- generateSlicedCookie(100,expected=TRUE)\n",
    "df <- df_orig %>% add_missinglabels_mar(Class~.,0.95)\n",
    "g_s <- SVM(Class~.,df,C=1,scale=TRUE,x_center=TRUE)\n",
    "g_s4 <- S4VM(Class~.,df,C1=1,C2=0.1,lambda_tradeoff = 3,scale=TRUE,x_center=TRUE)\n",
    "\n",
    "labs <- g_s4@labelings[-c(1:5),]\n",
    "colnames(labs) <- paste(\"Class\",seq_len(ncol(g_s4@labelings)),sep=\"-\")\n",
    "\n",
    "# Show the labelings that the algorithm is considering\n",
    "df %>%\n",
    "  filter(is.na(Class)) %>% \n",
    "  bind_cols(data.frame(labs,check.names = FALSE)) %>% \n",
    "  select(-Class) %>% \n",
    "  gather(Classifier,Label,-X1,-X2) %>% \n",
    "  ggplot(aes(x=X1,y=X2,color=Label)) +\n",
    "  geom_point() +\n",
    "  facet_wrap(~Classifier,ncol=5)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the final labeling that was selected\n",
    "# Note that this may not correspond to a linear classifier\n",
    "# even if the linear kernel is used.\n",
    "# The solution does not seem to make a lot of sense,\n",
    "# but this is what the current implementation returns\n",
    "df %>% \n",
    "  filter(is.na(Class)) %>% \n",
    "  mutate(prediction=g_s4@predictions) %>% \n",
    "  ggplot(aes(x=X1,y=X2,color=prediction)) +\n",
    "  geom_point() +\n",
    "  stat_classifier(color=\"black\", classifiers=list(g_s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The detailed code of module S4VM can be found at: https://rdrr.io/cran/RSSL/src/R/S4VM.R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some other useful materials, please also do these exercises by yourself.\n",
    "### * Semi-supervised learning in sklearn https://scikit-learn.org/stable/modules/label_propagation.html#semi-supervised\n",
    "### *Self training for image classification, with keras https://www.kaggle.com/glowingbazooka/semi-supervised-model-using-keras\n",
    "### *Semi-supervised learning book https://www.molgen.mpg.de/3659531/MITPress--SemiSupervised-Learning.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

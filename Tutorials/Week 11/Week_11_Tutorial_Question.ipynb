{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCR and PLS\n",
    "\n",
    "The materials used in this tutorial are based on the applied exercises provided in the book \"An Introduction to Statistical Learning with Applications in R\" (ISLR). We are trying to demonstrate how to implement the following methods to deal with the curse of dimensionality:\n",
    "* Principle Component Regression (PCR)\n",
    "* Partial Least Squares (PLS)\n",
    "    \n",
    "We have discussed that if the number of predictors is approximately equal to or larger than the number of observations, we can easily yield an over-fitted model. The problem of high dimension is very common in, for example, image processing, bio-informatics, etc. In the following exercises, we will show you how to use PCA and PLS.\n",
    "\n",
    "\n",
    "## 1. Predict the number of applications in the College data\n",
    "\n",
    "In the last tutorial, we how demonstrate how to penalize the linear regression with $L_1$ and $L_2$ regularization. In this exercise, we will predict the number of applications received using the other variables in the College data set with PCA and PLS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "library(ISLR)\n",
    "data(College)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Split the data set into a training set and a test set. \n",
    "As what we usually do, we should split the whole data set into a training set and a test set. We then fit our model on the training set and test its performance on the test set. Here, we randomly choose half of the data to be training data, the other half to be test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(1)\n",
    "# Randomly sample \n",
    "train = sample(1:dim(College)[1], dim(College)[1] / 2)\n",
    "test <- -train \n",
    "College.train <- College[train, ]\n",
    "College.test <- College[test, ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Fit a PCR model\n",
    "\n",
    "Fit a PCR model on the training set, with M chosen by cross-validation. Report the test error obtained, along with the value of M selected by cross-validation. In this task, the R library that you need is <a href=\"https://cran.r-project.org/web/packages/pls/pls.pdf\">pls</a>. If you would like to know more about the usage of pls, you can read Sections 3 to 8 of <a href=\"https://cran.r-project.org/web/packages/pls/vignettes/pls-manual.pdf\">\"Introduction to the pls Package\"</a>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(pls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A typical way of fitting a PCR model with cross-validation is to use the <font color=\"blue\">pcr()</font> function, to see the usage of the function please type "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(pcr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the usage of <font color=\"blue\">pcr()</font>, write your fitting code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit.pcr <- \n",
    "summary(fit.pcr) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the summary, there are two main results, namely the validation error and the cumulative percentage of variance explained using n components. The cross validation results are computed for each number of components used so that you can easily check the score with a particular number of components. The validation results are Root Mean Squared Error of Prediction (RMSEP). There are two cross-validation estimates: CV is the ordinary CV estimate, and adjCV is a bias-corrected CV estimate. Note that the percentage of variance explained in the predictors and in the response using different $M$ increases as $M$ increases.\n",
    "\n",
    "If is often simpler to judge the RMSEPs by plotting them, one way to do that\n",
    "is to use \n",
    "```r\n",
    "plot(RMSEP(fit.pcr), legendpos=\"topright\")\n",
    "```\n",
    "Instead, you can use the <font color=\"blue\">validationplot()</font> function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#help(validationplot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot shows that the smallest cross-validation error occurs when $M = 16$, which is only one less than the total number of predictors in the data set. We might think about just performing least squares.\n",
    "\n",
    "Now, we have found the lowest cross-validation error occurs when $M= 16$ components are used. We can compute the test MSE as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared with the MSE derived by Lasso and Ridge regression, the MSE given by PCR is larger. In this case, one might just choose to use either Lasso or Ridge regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3  FIT a PLS model\n",
    "Similar to what you have done with PCR, fit a PLS model on the training set, with M chosen by cross-validation. Report the test error obtained, along with the value of M selected by cross-validation.\n",
    "\n",
    "To do so, you need the <font color=\"blue\"> plsr()</font> function, and remember to print out the summary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit.pls <- \n",
    "summary(fit.pcr) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can also plot the MSEP as a function of $M$ with the <font color=\"blue\">validationplot()</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot shows that MSEP drops sharply while M changes from 1 to 5. After that, adding more component does not improve the MSEP too much. Here we are going to choose $M=10$ and compute the test MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.pls <- \n",
    "mean((pred.pls - College.test$Apps)^2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PLS model yielded lower MSE than did the PCR model.\n",
    "\n",
    "You have seen how to use the <font color=\"blue\">validationplot</font> to assess how many components are optimal. Actually, there are some other plots that we can use to diagnose the fitted models. For example,\n",
    "\n",
    "* Plots of the regression coefficients or loadings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(fit.pls, plottype=\"coef\", ncomp= 1:3, legendpos=\"topright\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above figure shows plots of the regression vectors for several different number of components at once. The plots describes how strongly each component in the PLSR depends on the original variables, and in what direction. I\n",
    "\n",
    "n the case of PCR, we can plot the loading vector for each component.\n",
    "* Score plots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(fit.pls, comps=1:3, plottype=\"scores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score plots are often used to look for patterns, groups or outliers in the data. The numbers in parentheses after the component labels are the relative amount of X variance explained by each component. The explained variances can be extracted explicitly with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explvar(fit.pls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more dicussions on inspecting fitted models, please refer to Section 7 of <a href=\"https://cran.r-project.org/web/packages/pls/vignettes/pls-manual.pdf\">\"Introduction to the pls Package\"</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  2. Predict per capital crime crate in the \"Boston\" data set\n",
    "\n",
    "In this exercise, we are going to investigate the methods discussed in this week and week 5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "require(\"MASS\")\n",
    "require(\"glmnet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, split the data set into a training set and a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(0)\n",
    "train = sample(c(TRUE,FALSE), dim(Boston)[1], rep=TRUE)\n",
    "test = (!train)\n",
    "\n",
    "Boston_train = Boston[train,]\n",
    "Boston_test = Boston[test,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the rest of the task, you are going to \n",
    "1. Try out some of the regression methods such as the lasso, ridge regression,  PCR and PLSR Present and discuss results for the approaches that you consider.\n",
    "1. Propose a model (or set of models) that seem to perform well on this data set, and justify your answer. Make sure that you are evaluating model performance using validation set error, cross-validation, or some other reasonable alternative, as opposed to using training error.\n",
    "1. Does your chosen model involve all of the features in the data set? Why or why not? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with Ridge regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, compute the test MSE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, let's test the Lasso model. You only need to chance <font color=\"brown\">alpha</font> to 1 in the <font color = \"blue\">cv.glmnet()</font> function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we try the PCR model by following what you have done in the first exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The validation plot looks suggests that we should use 3 predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's try to fit a PLS model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "The CV suggests the best $M$ is 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at the test MSE, which model does give the lowest test MSE?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

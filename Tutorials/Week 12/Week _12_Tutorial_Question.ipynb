{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning\n",
    "\n",
    "The materials used in this tutorial are based on the applied exercises provided in the book \"An Introduction to Statistical Learning with Applications in R\" (ISLR). We are trying to demonstrate how to implement the following unsupervised methods:\n",
    "* Hierarchical clustering\n",
    "* PCA\n",
    "* K-means\n",
    "\n",
    "Even though you will learn how to use the above methods, it is still worth trying by yourself\n",
    "* Sections 10.4, 10.5 and 10.6 in the textbook\n",
    "    \n",
    "\n",
    "# 1. Hierarchical clustering\n",
    "In this exercise, we are going to perform hierarchical clustering on the states of \n",
    "the <b><a href=\"https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/USArrests.html\">USArrests</a></b> data. It contains statistics in arrests in US states, i.e., the number of arrests per 100,000 residents for each of the following crimes: Assault, Murder and Rape. The textbook shows how we can use PCA to visualize the 50 states, and reveal some interesting patterns. Here, you are going to use a hierarchical clustering algorithm to find some interesting subgroups of states in regard to the crime statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "library(ISLR)\n",
    "set.seed(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1  Using hierarchical clustering with complete linkage and Euclidean distance to cluster the states.\n",
    "\n",
    "The method you are going to use to compute the inter-cluster dissimilarity is complete linkage (please refer to page 395 in the text book). The similarity measure is the Euclidean distance between two feature vectors. The R function used here is <b><i><a href=\"https://stat.ethz.ch/R-manual/R-devel/library/stats/html/hclust.html\">hclust()</a></i></b>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(hclust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot shows a Dendrogram, which looks like a tree. The leave nodes are the data points, i.e., the 50 states. The hierarchical algorithm starts from the leaves by merging together two leave nodes that are very similar to each other. The merging of clusters/nodes will proceed iteratively from the bottom to the top.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Cut the dendrogram at a height that results in three distinct clusters. \n",
    "\n",
    "In this exercise, we are interested in put all the states into three clusters, and print out which states belong to which clusters. To do this, you can use the <a href=\"https://stat.ethz.ch/R-manual/R-devel/library/stats/html/cutree.html\">cutree()</a> function. You need to set the desired number of groups <font color='brown'>k</font> to 3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output from the above code shows which cluster each state has been assigned to. For example, <font color=\"brown\">California</font> is assigned to cluster number 1. You can also visualize the group in the dendrogram as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the dendrogram together with Figure 10.1 in the text book. The analysis under figure 10.1 says that the first component roughly corresponds to a measure of overall rates of serious crimes. Figure 10.1 shows that states like <font color='brown'>California</font>, <font color='brown'>Nevada</font>, <font color='brown'>Florida</font>, <font color='brown'>Maryland</font>, <font color='brown'>Alaska</font> have high overall crime rate, and the dendrogram shows that they are all put into the same cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Scale variables before Hierarchical clustering. \n",
    "We have discussed how variable scale can affect both PCA and clustering algorithms. In terms of clustering algorithms, different variable scales could have a significant effect on computing the similarities between clusters or between data points. In this exercise, you are going to repeat what you have done in Exercises 1.1 and 1.2 by standardizing the data values, which can be done with the <a href=\"https://stat.ethz.ch/R-manual/R-devel/library/base/html/scale.html\"> scale()</a> function built-in R. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(USArrests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the <font color=\"blue\">apply()</font> function to compute the standard deviation of each variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The summary shows that variable values are in different scale. You should consider standardizing the values so that after standardizing the variables to have standard deviation one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also check the standard deviation of each variable after standardization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, use the hclust function to build the hierarchical clustering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if you compare the above dendrogram with the one generated in Exercise 1.1, the two three structures are different. It looks like we have four clusters, which is more fine-grained clustering of the 50 states. Again, let's jointly look at Figure 10.1 and the dendrogram generated above. We will find that the states in the first cluster from the right are very close to each other in Figure 10.1. A similar pattern can also be observed in the second cluster from the right. More discussion about what effect does scaling the variables have on the hierarchical clustering obtained can be found on page 398 in the textbook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA and K-means clustering\n",
    "\n",
    "In this exercise, you will generate simulated data, and then perform PCA and K-means clustering on the data. Firstly, generate a simulated data set with 20 observations in each of three classes (i.e., 60 observations in total), and 50 variables. There are a number of functions in R that you can use to generate data. One example is the <font color=\"blue\">rnorm()</font> function; <font color=\"blue\">runif()</font> is another option. Be sure to add a mean shift to the observations in each class so that there are three distinct classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#K = 3  # the number of classes\n",
    "set.seed(3)\n",
    "n = 20 # the number of samples per class\n",
    "p = 50 # the number of variables \n",
    "# Create data for class 1: \n",
    "X_1 = matrix( rnorm(n*p), nrow=n, ncol=p )\n",
    "for( row in 1:n ){\n",
    "  X_1[row,] = X_1[row,] + rep( 1, p ) \n",
    "}\n",
    "# Create data for class 2: \n",
    "X_2 = matrix( rnorm(n*p), nrow=n, ncol=p )\n",
    "for( row in 1:n ){\n",
    "  X_2[row,] = X_2[row,] + rep( -1, p ) \n",
    "}\n",
    "# Create data for class 3:\n",
    "X_3 = matrix( rnorm(n*p), nrow=n, ncol=p )\n",
    "for( row in 1:n ){\n",
    "  X_3[row,] = X_3[row,] + c( rep( +1, p/2 ), rep( -1, p/2 ) ) \n",
    "}\n",
    "X = rbind( X_1, X_2, X_3 )\n",
    "labels = c( rep(1,n), rep(2,n), rep(3,n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 PCA \n",
    "In this exercise, you will perform PCA on the 60 observations and plot the first two principal component score vectors. Use a different color to indicate the observations in each of the three classes. If the three classes appear separated in this plot, then continue on to part (c). If not, then return to part (a) and modify the simulation so that there is greater separation between the three classes. Do not continue to part (c) until the three classes show at least some separation in the first two principal component score vectors.\n",
    "\n",
    "The <font color = \"blue\">prcomp</font> function returns an object of class prcomp, which have some methods available. The <font color=\"blue\">print</font> method returns the standard deviation of each of the PCs, and their loadings, which are the coefficients of the linear combinations of the variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since scale (i.e., skewness and the magnitude) of the variables influence the resulting PCs, it is good practice to apply variable standardization, center and scale the variables prior to the application of PCA.\n",
    "\n",
    "The <font color=\"blue\">plot</font> method returns a plot of the variances (y-axis) associated with the PCs (x-axis). The Figure below is useful to decide how many PCs to retain for further analysis. It shows that the first 2 PCs explain most of the variability in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The summary method describe the importance of the PCs. The first row describe again the standard deviation associated with each PC. The second row shows the proportion of the variance in the data explained by each component while the third row describe the cumulative proportion of explained variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, plot the first two PCs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot shows that the three classes appear well separated. We can move to the next exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Perform K-means clustering of the observations with K = 3.\n",
    "\n",
    "How well do the clusters that you obtained in K-means clustering compare to the true class labels? Hint: You can use the <font color=\"blue\">table()</font> function in R to compare the true class labels to the class labels obtained by clustering. Be careful how you interpret the results: K-means clustering will arbitrarily number the clusters, so you cannot simply check whether the true class labels and clustering labels are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table shows that the k-mean clustering algorithm has successfully identified the three class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can try K-means clustering with K = 2. Describe your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the cluster outputs, you will find points in two classes are merged into one cluster. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, perform K-means clustering with K = 4, and describe your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output clearly indicates that points in one class has been split into two clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Now perform K-means clustering with K = 3 on the first two principal component score vectors. \n",
    "In this exercise, instead of running <a><font color=\"blue\">kmeans</font></a> on the raw data, you will perform K-means clustering on the 60 Ã— 2 matrix of which the first column is the first principal component score vector, and the second column is the second principal component score vector. Comment on the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outputs above show a perfect match between clusters and classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Using the scale() function, perform K-means clustering with K = 3 on the data after scaling each variable to have standard deviation one. How do these results compare to those obtained in (b)? Explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# More reference reading materials\n",
    "* <a href=\"http://www.statmethods.net/advstats/cluster.html\">Cluster Analysis</a> provided on the Quick-R website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

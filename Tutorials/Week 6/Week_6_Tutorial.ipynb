{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Model Selection and Regularization\n",
    "\n",
    "\n",
    "The materials used in this tutorial are based on the applied exercises provided in the book \"An Introduction to Statistical Learning with Applications in R\" (ISLR). We are trying to demonstrate how to implement the following methods of selecting the most appropriate model that explains the data:\n",
    "    * Subset selection\n",
    "    * Shrinkage method\n",
    "        * Ridge regression (L-2 regularization)\n",
    "        * Lasso (L-1 regularization)\n",
    "\n",
    "Even though you will learn how to use the above methods, it still is worth trying by yourself\n",
    "    1. Sections 6.5 and 6.6\n",
    "    1. The other practical exercises in section 6.8 (optional)\n",
    "    \n",
    "The libraries that you needed are\n",
    "* <a href=\"https://cran.r-project.org/web/packages/leaps/index.html\">leaps</a>\n",
    "* <a href=\"https://cran.r-project.org/web/packages/glmnet/index.html\">glmnet</a>\n",
    "    \n",
    "## 1. Subset Selection, Stepwise Selection and Lasso for selection features\n",
    "\n",
    "Subset selection is an approach of identifying a subset of predictors (or attributes) that we believe have strong an association with the response variable. In this exercise, we will generate simulated data, and will use this data to perform best subset selection.\n",
    "\n",
    "### 1.1 Generate simulated data\n",
    "Here, we are going to use the <a href=\"https://stat.ethz.ch/R-manual/R-devel/library/stats/html/Normal.html\">rnorm()</a> function to generate a predictor $X$ of length $n = 100$, as well as a noise vector of length $n = 100$. Note that we start with setting the seed for the random number generate, the purpose of which is to make our analysis reproducible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "set.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first generate 100 random values that are normally distributed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, generate some random noises as irreducible errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can generate a response vector $Y$ of length $n = 100$ according to the model \n",
    "    $Y = \\beta_0+\\beta_1 X +\\beta_2 X^2 +\\beta_3 X^3 + \\epsilon $ \n",
    "where $\\beta_0$, $\\beta_1$, $\\beta_2$ and $\\beta_3$  are constants of your choice.\n",
    "Here, we will set those coefficients as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b0 <- 2\n",
    "b1 <- 3\n",
    "b2 <- -1\n",
    "b3 <- 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the coefficients, it is not hard to generate the simulated data using the polynomial function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y <- b0 + b1 * x + b2 * x^2 + b3 * x^3 + eps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2  Perform best subset selection \n",
    "In this task, we will use the regsubsets() function (refer to the documentation of <a href=\"https://cran.r-project.org/web/packages/leaps/leaps.pdf\">leaps</a> for the function specification) to perform best subset selection in order to choose the best model containing the predictors $X$, $X^2$, $\\dots$, $X^{10}$. What is the best model obtained according to $C_p$, $BIC$, and adjusted $R^2$? Show some plots to provide evidence for your answer, and report the coefficients of the best model obtained. Note you will need to use the <a href=\"https://stat.ethz.ch/R-manual/R-devel/library/base/html/data.frame.html\">data.frame()</a> function to create a single data set containing both $X$ and $Y$.\n",
    "\n",
    "We start with loading the library,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "library(leaps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, put our data into a data frame,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.full <- data.frame(y = y, x = x)\n",
    "data.full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's have a look at our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(data.full$x, data.full$y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the plot shows the relationship between $X$ and $Y$ is linear. However, it is indeed not, as our data were generated from a polynomial function with degree 3. Our goal here is to find the best model contains at most 10 variables, from $X$ to $X^{10}$. The first thing that you need to do is to call the <font color=\"blue\"> regsubsets()</font> function. To see the documentation of this function, simply type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(regsubsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above documentation tells us that the first argument is the model formula for the full model, which is\n",
    "$\\hat{Y} = \\beta_0 + \\beta_1 X + \\beta_2 X^2 + \\dots + \\beta_{10} x^{10}$. We should also specify the data and the maximum size of subset to examine, which is 10 here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regfit.full <- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The summary() function will output the best set of variables for each model size from 1 to 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.summary <- summary(regfit.full)\n",
    "reg.summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An asterisk indicates that a variable in the corresponding column is included in the model. For example, the best one-variable model only contains $X$and  the best three-variable model contains $X$, $X^2$ and $X^5$. As discussed in section 6.5.1 in the text book, ISLR, the <font color=\"orange\">nvmax</font> option can be used in order to return as many as variables as are desired, subject to $nvmax <= p$. Besides, what has been show above, the <font color=\"blue\">summary()</font> function also returns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in eval(expr, envir, enclos): object 'reg.summary' not found\n",
     "output_type": "error",
     "traceback": [
      "Error in eval(expr, envir, enclos): object 'reg.summary' not found\nTraceback:\n"
     ]
    }
   ],
   "source": [
    "names(reg.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can examine these outputs to identify the best overall model. First, let's find the best overall model using Mallow's CP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(reg.summary$cp, xlab = \"Number of variables\", ylab = \"C_p\", type = \"l\")\n",
    "mincp = which.min(reg.summary$cp)\n",
    "points(mincp, reg.summary$cp[mincp], col = \"red\", cex = 2, pch = 20)\n",
    "mincp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot suggests that the best overall model is a model with 3 variables, what are the three variables and their coefficients?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef(regfit.full, mincp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can use some other criteria, such as BIC and RSS. Let's try BIC first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BIC gives us the same model as did CP. How about using adjusted $R^2$,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is interesting that the three criteria gave us the the same overall best model. We can further have a look at the RSS plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(reg.summary$rss, xlab = \"Number of variables\", ylab = \"RSS\", type = \"l\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we observed is that RSS decrease monotonicity as the number variable increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Perform stepwise selection\n",
    "\n",
    "One drawback of best subset selection is its time complexity. In general, the number of models that you need to fit is $2^p$, which is infeasible if $p$ is large, for example, if $p =10$. It also easily results in a model that fits the training data perfectly but performing badly on unseen data. In this section, we are going to explore alternative methods, known as stepwise selection, which explore a far more restricted set of models. There are three stepwise selection methods:\n",
    "* <b>Forward stepwise selection</b>: Starts with one-variable models, gradually add one variable, end with a model including all the specified variables.\n",
    "* <b>Backward stepwise selection</b>: Starts with a full model, gradually exclude one variable, end with one-variable models.\n",
    "* <b>Hybrid selection</b>: the combination of Forward stepwise selection and Backward stepwise selection.\n",
    "\n",
    "We begin with forward stepwise selection. It is very easy to implement the forward stepwise selection method with <font color=\"blue\">regsubsets()</font>: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regfit.fwd <-  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "reg.summary.fwd <-\n",
    "reg.summary.fwd \n",
    "reg.summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have you observed the differences between the above output and that generated by the best subset selection? For example, is the five-variable model generated by the forward stepwise selection the same as the one generated by the best subset selection? \n",
    "\n",
    "The answer is No. the five-variable model generated by the forward stepwise selection contains $X$, $X^2$, $X^4$, $X^5$ and $X^9$. However, the one generated by the best subset selection contains $X$, $X^2$, $X^5$, $X^8$ and $X^9$. You can also find that the 6,7,8-variable models are also different.\n",
    "\n",
    "Now, what is the overall best model? Similarly, we can generate a set of plots to identify the best overall model as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par(mfrow = c(2, 2))\n",
    "plot(reg.summary.fwd$cp, xlab = \"Number of variables\", ylab = \"C_p\", type = \"l\")\n",
    "points(which.min(reg.summary.fwd$cp), reg.summary.fwd$cp[which.min(reg.summary.fwd$cp)], col = \"red\", cex = 2, pch = 20)\n",
    "plot(reg.summary.fwd$bic, xlab = \"Number of variables\", ylab = \"BIC\", type = \"l\")\n",
    "points(which.min(reg.summary.fwd$bic), reg.summary.fwd$bic[which.min(reg.summary.fwd$bic)], col = \"red\", cex = 2, pch = 20)\n",
    "plot(reg.summary.fwd$adjr2, xlab = \"Number of variables\", ylab = \"Adjusted R^2\", type = \"l\")\n",
    "points(which.max(reg.summary.fwd$adjr2), reg.summary.fwd$adjr2[which.max(reg.summary.fwd$adjr2)], col = \"red\", cex = 2, pch = 20)\n",
    "plot(reg.summary.fwd$rss, xlab = \"Number of variables\", ylab = \"RSS\", type = \"l\")\n",
    "mtext(\"Plots of C_p, BIC, adjusted R^2 and RSS for forward stepwise selection\", side = 3, line = -2, outer = TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above plots show that the best overall model is still the 3-variable model. How about the coefficients?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef(regfit.fwd, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will find the estimated coefficients are the same.\n",
    "\n",
    "Now, let's have a look at the backward selection method. To implement it, we just simply, change the value of <font color=\"orange\">method</font> to <b>backward</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regfit.bwd <- \n",
    "reg.summary.bwd <- \n",
    "reg.summary.bwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you identify the differences between the models selected by the backward selection method and those selected by the forward selection method?\n",
    "\n",
    "Similarly, we can find the best overall model given by the backward selection method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par(mfrow = c(2, 2))\n",
    "plot(reg.summary.bwd$cp, xlab = \"Number of variables\", ylab = \"C_p\", type = \"l\")\n",
    "points(which.min(reg.summary.bwd$cp), reg.summary.bwd$cp[which.min(reg.summary.bwd$cp)], col = \"red\", cex = 2, pch = 20)\n",
    "plot(reg.summary.bwd$bic, xlab = \"Number of variables\", ylab = \"BIC\", type = \"l\")\n",
    "points(which.min(reg.summary.bwd$bic), reg.summary.bwd$bic[which.min(reg.summary.bwd$bic)], col = \"red\", cex = 2, pch = 20)\n",
    "plot(reg.summary.bwd$adjr2, xlab = \"Number of variables\", ylab = \"Adjusted R^2\", type = \"l\")\n",
    "points(which.max(reg.summary.bwd$adjr2), reg.summary.bwd$adjr2[which.max(reg.summary.bwd$adjr2)], col = \"red\", cex = 2, pch = 20)\n",
    "plot(reg.summary.bwd$rss, xlab = \"Number of variables\", ylab = \"Adjusted R^2\", type = \"l\")\n",
    "mtext(\"Plots of C_p, BIC, adjusted R^2 and RSS for backward stepwise selection\", side = 3, line = -2, outer = TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best overall model is still the three-variable model with exactly the same variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Perform Lasso\n",
    "What we have done so far is about selecting the best subset of variables used in the model, which is often known as feature selection. As discussed in the lecture, the Lasso is one approach that we often used in machine learning to perform feature selection automatically. Depending on the shrinkage parameter, Lasso regularizes the coefficient in a way such that the estimated coefficients can be shrunk toward zero. The R library that we are going to use is the <font color=\"orange\">glmnet</font>. It's document table can be found <a href=\"https://cran.r-project.org/web/packages/glmnet/glmnet.pdf\">here</a>. Remember that it is always a good idea to refer to the library documentation to check the usage of a given function. The main function that we are going to use is <font color=\"blue\">glmnet()</font>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(glmnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(glmnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to fit a lasso model to the simulated data, again using $X$,$X_2$, $\\dots$, $X_{10}$ as predictors. Use cross-validation to select the optimal value of $\\lambda$. Create plots of the cross-validation error as a function of $\\lambda$. Report the resulting coefficient estimates, and discuss the results obtained.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xmat <- model.matrix(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6) + I(x^7) + I(x^8) + I(x^9) + I(x^10), data = data.full)[, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed in section 6.6, the <a href=\"https://stat.ethz.ch/R-manual/R-devel/library/stats/html/model.matrix.html\">model.matrix</a> function produces a data matrix corresponding to the 10 variables and also transforms any qualitative variables into dummy variables. In this case, we don't have qualitative variables. You should note that <font color=\"blue\">glmnet()</font> can only take numerical, quantitative inputs.\n",
    "\n",
    "Now, we fit a lasso model with cross-validation using the <font color=\"blue\">cv.glmnet()</font> function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv.lasso <-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If <font color=\"orange\">alpha</font> is set to 0, a Ridge regression model is going to be fit. The default number of folds used is 10. You can also set the number of folds by specifying the value of <font color=\"orange\">nfolds</font>. To see more detailed usage of cv.glmnet, simply type\n",
    "```R\n",
    "help(cv.glmnet)\n",
    "```\n",
    "\n",
    "We can plot the MSE as a function of the logarithm of $\\lambda$ with error bars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(cv.lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the cross-validated lambda value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestlam <- cv.lasso$lambda.min\n",
    "bestlam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we refit our lasso model using $\\lambda$ chosen by cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit.lasso <- glmnet(xmat, y, alpha = 1)\n",
    "predict(fit.lasso, s = bestlam, type = \"coefficients\")[1:11, ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above result shows that 6 of the 10 coefficient estimates are exactly zero. So the lasso model with $\\lambda$ chosen by cross-validation contains only four variable, besides the intercept. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2. Lasso v.s. Ridge Regression\n",
    "\n",
    "In machine learning, we alos call Lasso as $L_1$ regularizition and Ridge as $L_2$ regularization. The difference between the two regularization methods lies in how they penalise the estimated parameters of your model. The Ridge regularization will shrink all the estimated parameters twowards zero, but never equal to zero. In contrast, the Lasso regularization will force some of the estimated parameters to be zero. Besides the two regularization methods, there are other regularization methods, such as\n",
    "* <a href=\"http://statweb.stanford.edu/~tibs/ftp/sparse-grlasso.pdf\">group lasso</a>, which regularize the estimated parameters at a group level. Note you don't need to understand it, we just would like to let you be aware of the other regularization methods that are also used in data analysis. \n",
    "\n",
    "In this exercise, you are going to study how the Lasso and Ridge regularization \n",
    "affect the perforamnce of linear regression. We will develop a linear model to predict the number of applications received using the other variables in the <font color=\"orange\">College</font> data set. We begin with splitting the data set into a training set and a testing set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(ISLR)\n",
    "data(College)\n",
    "set.seed(1)\n",
    "dim(College)\n",
    "train = sample(1:dim(College)[1], dim(College)[1] / 2)\n",
    "test <- -train\n",
    "College.train <- College[train, ]\n",
    "College.test <- College[test, ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Remeber that if you would like to check the specification of a R function, you can alwasy call the <font color=\"blue\">help()</font> function. Here, we randonly select a half of the data records for training the linear model, and the other half to be in the testing data set.\n",
    "\n",
    "### 2.1 Fit a least square model.\n",
    "For the comparsion purpose, we first fit a linear model using least squares on the training set, and report the test error obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fit.lm <- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred.lm <- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean((pred.lm - College.test$Apps)^2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The means quare error is quite large. \n",
    "\n",
    "### 2.2 Reguarlize the coefficients with the $L_2$ regularization\n",
    "Now, we are going to fit a ridge regression model on the training set, with the shrinkage parameter (or tunning) $\\lambda$ chosen by cross-validation. Remember that the training data taken by the <font color=\"blue\">glmnet</font> function (Type <font color=\"blue\">help(glmnet)</font> to see its specification) should be a matrix. So we have to store all the training and testing datasets in two matrices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.mat <- model.matrix(Apps ~ ., data = College.train)[,-1]\n",
    "test.mat <- model.matrix(Apps ~ ., data = College.test)[,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we generate a list of lambda values that will be used in cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid <- 10^seq(4, -2, length = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the generated list of possible lambda values, we can now fit the lasso model with the cv.glmnet function. Type \n",
    "```r\n",
    "help(cv.glmnet)\n",
    "```\n",
    "to see the usage information of this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(1)# the purpose of fixing the seed of the random number generator is to make the result repeatable.\n",
    "fit.ridge <- \n",
    "cv.ridge <- \n",
    "bestlam.ridge <- cv.ridge$lambda.min\n",
    "bestlam.ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.ridge <- predict(fit.ridge, s = bestlam.ridge, newx = test.mat)\n",
    "mean((pred.ridge - College.test$Apps)^2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Reguarlize the coefficients with the $L_1$ regularization\n",
    "In this question, we will fit a lasso model on the training set, with λ chosen by cross-validation. Report the test error obtained, along with the number of non-zero coefficient estimates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(1)# the purpose of fixing the seed of the random number generator is to make the result repeatable.\n",
    "fit.lasso <- \n",
    "cv.lasso <- \n",
    "bestlam.lasso <- cv.lasso$lambda.min\n",
    "bestlam.lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.lasso <- \n",
    "mean((pred.lasso - College.test$Apps)^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(fit.lasso, s = bestlam.lasso, type = \"coefficients\")[1:17, ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we compare the three models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean((pred.lm - College.test$Apps)^2)\n",
    "mean((pred.ridge - College.test$Apps)^2)\n",
    "mean((pred.lasso - College.test$Apps)^2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lowest MSE is drived by the lasso model.\n",
    "\n",
    "\n",
    "Optional readings:\n",
    "https://rstudio-pubs-static.s3.amazonaws.com/2897_9220b21cfc0c43a396ff9abf122bb351.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

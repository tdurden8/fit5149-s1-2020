{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree-based Methods\n",
    "\n",
    "The materials used in this tutorial are based on the applied exercises provided in the book \"An Introduction to Statistical Learning with Applications in R\" (ISLR). We are trying to demonstrate how to implement the following tree based method:\n",
    "\n",
    "* Decision tree\n",
    "* Random forest \n",
    "* Bagging \n",
    "\n",
    "\n",
    "First you need to install libraries\n",
    "\n",
    "```r\n",
    "install.packages(\"tree\")\n",
    "install.packages(\"randomForest\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load all the libraries\n",
    "require(tree)\n",
    "require(randomForest)\n",
    "library(glmnet)\n",
    "library(gbm)\n",
    "library(ISLR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1. Predict purchase in OJ dataset\n",
    "In this question, we will treat <font color=\"brown\">Purchase</font> as the response in <font color=\"brown\">OJ</font> dataset. This dataset consists of 1070 observations on the following 18 variables. (The following detail is copied and pasted from <a href=\"https://cran.r-project.org/web/packages/ISLR/ISLR.pdf\">here</a>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?OJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head(OJ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table(OJ$Purchase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Create a training set containing a random sample of 800 observations, and a test set containing the remaining observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "set.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate training samples,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit a classification tree,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tree only uses four variables \n",
    "* LoyalCH\n",
    "* PriceDiff\n",
    "* SpecialCH\n",
    "* ListPriceDiff\n",
    "\n",
    "They are the important predictors that are used to construct the decision tree. \n",
    "\n",
    "The train error rate is 0.165 for this classification tree. There are 8 terminal nodes in this tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Create a text output for the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pick terminal node labeled “11)”. The splitting variable at this node is 𝙿𝚛𝚒𝚌𝚎𝙳𝚒𝚏𝚏. \n",
    "\n",
    "* The splitting value of this node is 0.195. \n",
    "* There are 101 points in the subtree below this node. \n",
    "* The deviance for all points contained in region below this node is 139.20. \n",
    "* A * in the line denotes that this is in fact a terminal node. \n",
    "* The prediction at this node is 𝚂𝚊𝚕𝚎𝚜 = CH. \n",
    "* About 54.4% points in this node have 𝙲𝙷 as value of 𝚂𝚊𝚕𝚎𝚜. Remaining 45.5% points have 𝙼𝙼 as value of 𝚂𝚊𝚕𝚎𝚜."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Create a plot of the tree, and interpret the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "𝙻𝚘𝚢𝚊𝚕𝙲𝙷 is the most important variable of the tree, in fact top 3 nodes contain 𝙻𝚘𝚢𝚊𝚕𝙲𝙷. \n",
    "\n",
    "If 𝙻𝚘𝚢𝚊𝚕𝙲𝙷<𝟶.𝟸64, the tree predicts 𝙼𝙼. \n",
    "\n",
    "If 𝙻𝚘𝚢𝚊𝚕𝙲𝙷>𝟶.𝟽𝟼5, the tree predicts 𝙲𝙷. \n",
    "\n",
    "For intermediate values of 𝙻𝚘𝚢𝚊𝚕𝙲𝙷, the decision also depends on the value of 𝙿𝚛𝚒𝚌𝚎𝙳𝚒𝚏𝚏.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Predict the response on the test data, and produce the confusion matrix comparing the test labels to the predicted test labels. What is the error rate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Apply the <font color=\"blue\">cv.tree()</font> function to the training set in order to determine the optimal tree size. Produce plots with tree size and cross-validation classification error rate. Which tree size is chosen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best size number are 2, 5 and 8. Let's choose 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Produce a pruned tree corresponding to the optimal tree size obtained using cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 Compare the training error rates between the pruned and unpruned trees. Which is higher?\n",
    "\n",
    "You can use the summary() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train error for both classification trees are the same at 0.165, but the residual mean deviance for the pruned tree is higher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8 Compare the testing error rates between the pruned and unpruned trees. Which is higher?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2. Use boosting to predict Salary in the Hitters dataset \n",
    "\n",
    "In this task, we are going to study how to fit a boosted regression tree.\n",
    "\n",
    "### 2.1 Prepare the training and the testing datasets.\n",
    "There are some observations for whom the salary information is unknown. We need to exclude those observations from the datasets. And then, log-transform the salaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(is.na(Hitters$Salary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hitters <- na.omit(Hitters)\n",
    "Hitters$Salary <- log(Hitters$Salary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As what you have done in the task 1 above, we generate the training and testing splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train <- 1:200\n",
    "Hitters.train <- Hitters[train, ]\n",
    "Hitters.test <- Hitters[-train, ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Fit a boosted tree \n",
    "Perform boosting on the training set with 1000 trees for a range of values of the shrinkage parameter $\\lambda$.\n",
    "Produce twos plots:\n",
    "* one with different shrinkage values on the x-axis and the corresponding training set MSE on the y-axis.\n",
    "* one with different shrinkage values on the x-axis and the corresponding testing set MSE on the y-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pows = seq(-10, -0.2, by = 0.1)\n",
    "lambdas = 10^pows\n",
    "length.lambdas = length(lambdas)\n",
    "train.errors = rep(NA, length.lambdas)\n",
    "test.errors = rep(NA, length.lambdas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, perform the boosting with the setting above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (i in 1:length.lambdas) {\n",
    "#Write you code here\n",
    "    \n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the training/testing MSE as a function of $\\lambda$s results below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "par(mfcol=c(1,2),  pty = \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the min test error?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(test.errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas[which.min(test.errors)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Comparison\n",
    "\n",
    "Compare the test MSE of boosting to the test MSE that results from applying the regression approaches: \n",
    "* regression \n",
    "* lasso regression\n",
    "* Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x <- model.matrix(Salary ~ ., data = Hitters.train)\n",
    "x.test <- model.matrix(Salary ~ ., data = Hitters.test)\n",
    "y <- Hitters.train$Salary\n",
    "#Write your ridge regression code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write your lasso regression code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both linear model and regularization like Lasso have higher test MSE than boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Which variables appear to be the most important predictors in the boosted model ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may see that “CAtBat” is by far the most important variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Now apply bagging to the training set. What is the test set MSE for this approach ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "set.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test MSE for bagging is 0.23, which is slightly lower than the test MSE for boosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
